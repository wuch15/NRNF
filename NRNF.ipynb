{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZbEWTJbzQ8B"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "with open('train.tsv') as f:\n",
    "    trainuser=f.readlines()\n",
    "\n",
    "with open('test.tsv') as f:\n",
    "    testuser=f.readlines()\n",
    "\n",
    "with open('docs.tsv') as f:\n",
    "    data=f.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AA3fgHS',\n",
       " 'autos',\n",
       " 'autosnews',\n",
       " 'First Drive: 2009 Nissan GT-R',\n",
       " 'See the 2009 Nissan GT-R in action at Nissan\\'s test facility in Hokkaido, Japan: First Drive: Part 1 - 2009 Nissan GT-R Video First Drive: Part 2 - 2009 Nissan GT-R Video Scene One. Sendai Raceway, two hours north of Tokyo on the Bullet Train. Sendai is the only public racing circuit where Nissan tested its new GT-R. Apart from the Nurburgring in Germany-around which it\\'s the world\\'s fastest production car (only the limited-build Porsche Carrera GT laps faster, says Nissan). Sendai is like a mini \\'Ring: fast and undulating and full of sweeping corners and heart-in-mouth turns. It\\'s a roller-coaster raceway. \"We didn\\'t test on any other public racetrack because most are too easy,\" says chief engineer Kazutoshi Mizuno, ex-head of Nissan\\'s sports-car racing program. Press the red start button on the center console, and the 3.8-liter V-6 burbles awake, slightly muted by its twin turbos. It\\'s a new engine, not a steroidal version of the familiar VQ family of V-6s. The spec sheet says 473 horsepower. Soon we\\'ll discover you can feel every single one of them. But first, time to set up the car. Computer-game geeks will know the routine. It\\'s pure PlayStation. The bottom of the center of the dash features three switches. The left one adjusts the transmission-a six-speed paddleshift via a twin-clutch rear transaxle. Choose the R (for racing) mode for faster gearshifts. (You could choose normal shift speed. Or full automatic-in which case your GT-R can tootle along like a Lincoln Town Car). The middle switch controls the dampers. Again, choose R for race. Those Bilsteins are now on max-attack mode, rock hard. Alternatively, choose Sport or Comfort, the latter replacing the rocks in the dampers with fluffy cotton wool. Finally, turn the VDC (Vehicle Dynamic Control) also to R. Which means it\\'s turned off. All those electronic nannies stopping slides and skids and spinning tires are bypassed. You\\'re on your own. So the GT becomes a GT-R. The world\\'s fastest sub-$100,000 car; $75,000 to $80,000 is the likely price range. A car that makes even the hottest Corvette seem tame and crude. A car, to quote Nissan\\'s promo blurb, for \"anyone, anywhere, any time.\" Translation: Any sucker can drive this car fast anywhere. But we\\'re not through with the videogame bit just yet. Note the \"multifunction meter\" high in the center of the dash. It\\'s designed by the same guys who did the Gran Turismo graphics for Sony. So much information, just the sort you can get on your TV screen as you lap the Nurburgring in virtual reality. The myriad mechanical and driving information includes water temperature, oil pressure, turbo boost, instant fuel consumption, throttle opening, and-how\\'s this for gadgetry?-steering angle. There\\'s even a g-force meter. Not that you\\'ve got any time to look at it. Give the accelerator a prod, and the engine note turns baritone and barks. There\\'s a distant turbo whistle, helping hands on the way. Push the chunky, old-fashioned gear lever-standard auto transmission ware-down and across the dogleg into Drive, selecting the Manual rather than the Automatic transmission mode. Click the right paddle behind the steering wheel to select first. The GT-R does a little hiccup forward. You burble down the pit lane, those low-profile Bridgestones and rock-hard R-for-race-setting Bilsteins relaying every pebble and squashed cigarette butt to the soft-rimmed leather-wrapped steering wheel. You give the right pedal a big prod, the V-6 deepens in tone and growls, and then you\\'re out on the track and into second, and third, before a wickedly tight chicane-right, left-gives you a hint of what lies ahead. There are a couple of left-handers, then a longish back straight-into third, fourth, turbos kicking in brutally-and then you\\'re back on those big Brembo brakes that allegedly give the GT-R the best stopping power of any production car ever homologated in Japan. Better than any Ferrari or Porsche, Mizuno has said earlier, fatherly pride glowing. You spear through another chicane and you\\'re climbing up a hill, revs building toward the 7000-rpm peak-V-6 thundering in front of your toes, and you\\'d swear you were in a specially honed track racer-and there\\'s a series of fast corners, swish-swish through them with no hint of body roll. It\\'s obvious even so early in our test that this car offers scintillating performance (0 to 62 in 3.6 seconds, top speed 193 mph) and yet is so easy to drive. The power, as those big IHI turbos energize the V-6, frightens and stimulates, yet the reassuring computer-controlled four-wheel-drive system, those big brakes, and the sheer normalness of much of the driving experience-good visibility, conventional sedanlike driving position, conservative interior that could just as easily clothe the inside of a staid coupe as a superstar supercar-all exude confidence. The gearshifts are lightning fast-0.2 second after finger flick, torque is engaged to all four wheels-and the steering is sharp and linear. Later, you drive a BMW M6 that Nissan helpfully has on hand, and there\\'s no doubt the GT-R steers sharper (as well as noticeably faster). There\\'s no understeer to speak of; no front-end scrub even if you plow into a corner a touch too fast. Rather, the car does a nicely balanced sideways shimmy, accompanied by a tire chirp, and quickly gains control before you accelerate hard and all four tires claws at the tarmac and you\\'re spearing forward again. The performance is fuss-free. That\\'s what\\'s so special about this car: the way you simply steer and the way it obediently follows -- you guide it rather than grapple with it-and the flat cornering and the drama-free way it powers out of bends, engine note interrupted only for a fraction of a second as you swap gears fast and accurate, acceleration not interrupted at all. It\\'s a bit like a video-game supercar. Maybe that\\'s the only downside of the GT-R. It\\'s awesomely efficient, brutally fast. But it doesn\\'t serenade you with its steering, doesn\\'t charm you with its driving demeanor like a similar performance (for double the money!) Porsche or Ferrari. Even a BMW M3, though slower, has more mechanical poetry, especially when its tuneful V-8 is bearing down on 8400 rpm. Scene Two. A normal public road near Sendai Raceway. Time to calm down. Use those videogame switches to choose max comfy and full auto transmission. Relax. Turn up the radio. The engine still burbles, the power muted. You can even look at those Gran Turismo gauges and maybe dream about tonight\\'s PlayStation attempt finally to beat your son around Suzuka. The ride is firm yet comfy. There\\'s minimal wind noise. It\\'s almost quiet. The seats support. And if it snows or rains? No problema. Easy driving, comfortable driving. Enjoy the flip side of one of the world\\'s great cars-a true gran turismo with supercar performance. And when you want to go back to max attack, you know you\\'re only a few videogame-like switches and a stomp on the accelerator pedal away. 2009 NISSAN GT-R Base price $69,850 Vehicle layout Front engine, AWD, 4-pass, 2-door, coupe Engine 3.8L/480-hp/434-lb-ft twin turbo DOHC 24-valve V-6 Transmissions 6-speed; auto-clutch manual Curb weight 3836 lb (mfr) Wheelbase 109.4 in Length x width x height 183.2 x 74.6 x 53.9 in 0-60 mph 3.5 sec (mfr est) EPA city/hwy fuel econ 15/21 mpg (est) CO2 emissions 1.13 lb/mile (est) On sale in U.S. June 2008\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].split('\\t')#news id, category, subcategory, title, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['da04f164-2374-4f9d-8ace-d790933bd799',\n",
       " '2B482CDF0A419E1916E923C5FFFFFFFF',\n",
       " 'A',\n",
       " 'BBPCEYp BBPClcH BBPD6fE BBQ0LCa BBQG1DH',\n",
       " '57 98 46 66 47',\n",
       " '11/12/2018 1:29:18 PM#TAB#11/12/2018 1:30:16 PM#TAB#11/12/2018 1:31:55 PM#TAB#11/23/2018 7:25:17 AM#TAB#12/8/2018 1:15:25 PM',\n",
       " 'BBS7KRx BBS3rAy',\n",
       " 'BBS7xCn BBQYnJv BBS84oG BBS2YYC BBQRFKa BBS7GgD BBS9qn3 BBS7SDi BBS7LoC BBS9BgN BBS6yA2 BBS8WTT BBS9C6B BBS9Hy6 BBS8VsJ BBS8b6P BBS8kkd BBS7owY BBS7zxt BBS9xtw BBS7yFI BBS7uIF BBS7Kon BBS7Wr4 BBS4s5T BBS5VbF BBR8NJB BBS7Zww BBS86YX BBS84hE BBS7EzE BBS8bk0 BBS9wm9 BBS9nKG BBS58Om BBS7zRe BBRRrXl BBS6gG9 BBS8JfV BBS3D6t BBS8UZe BBS1ow3 BBS0SSm BBRIKPQ BBQsVjC BBS9BS2 BBS9GoG BBS9vfd BBS9suJ BBS9tBY BBS7fpA AAylz6W BBS3OBJ BBRhhYc BBS9BBL BBS5rf9 BBROoYP BBS9rw7 BBRYlEk BBS7DDs BBS9E8M BBS7lNI BBQqesC BBRUBjc BBS3KH4 BBS24s5 BBS8kBZ BBS9fyx BBS8TQp BBPIkbm BBS7PFG BBS8LQy BBS4VuF BBS8sYY',\n",
       " '1/12/2019 1:52:14 PM',\n",
       " '1/12/2019 2:31:46 PM\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainuser[0].split('\\t')#impression id, user id, user type, news type, clicked news' dwell time/s, timestamp, clicked samples, unclicked samples, session start, session end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk-pFDiaS1Fo"
   },
   "outputs": [],
   "source": [
    "newsindex={'NULL':0}\n",
    "for i in news:\n",
    "    newsindex[i]=len(newsindex)\n",
    "\n",
    "def newsample(x,ratio):\n",
    "    if ratio >len(x):\n",
    "        return random.sample(x*(ratio//len(x)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(x,ratio)\n",
    "\n",
    "\n",
    "news={}\n",
    "category={}\n",
    "subcategory={}\n",
    "for i in data:\n",
    "    line=i.strip().split('\\t')\n",
    "    news[line[0]]=[line[1],line[2],word_tokenize(line[3]),word_tokenize(line[4])]\n",
    "    category[line[1]]=0\n",
    "    subcategory[line[2]]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk-pFDiaS1Fo"
   },
   "outputs": [],
   "source": [
    "npratio=4\n",
    "time_T=10\n",
    "train_sample=[]    \n",
    "label=[]\n",
    "user_pos=[]\n",
    "user_neg=[]\n",
    "\n",
    "\n",
    "for line in trainuser:\n",
    "  \n",
    "    line=line.replace('\\n','').split('\\t')\n",
    "  \n",
    "    clickids=[news[x] for x in line[3].split()][-50:]\n",
    "    dwelltime=[int(x) for x in line[4].split()][-50:]\n",
    "    posfeedback=[clickids[x] for x in range(len(dwelltime)) if dwelltime[x] >time_T ]   \n",
    "    negfeedback=[clickids[x] for x in range(len(dwelltime)) if dwelltime[x] <=time_T ][:20]    \n",
    "    pdoc=[newsindex[x] for x in line[6].split()]\n",
    "    ndoc=[newsindex[x] for x in line[7].split()]\n",
    "    \n",
    "    for pos in pdoc:\n",
    "  \n",
    "        neg_samples=newsample(ndoc,npratio)\n",
    "        all_samples=neg_samples+[pos]\n",
    "        concat_label=[0]*npratio+[1]\n",
    "        train_sample.append(all_samples)\n",
    "        label.append(concat_label)\n",
    "        user_pos.append(posfeedback+[0]*(50-len(posfeedback)))\n",
    "        user_neg.append(negfeedback+[0]*(20-len(negfeedback)))\n",
    "\n",
    "\n",
    "test_sample=[]    \n",
    "test_label=[]\n",
    "test_user_pos=[]\n",
    "test_user_neg=[]\n",
    "test_index=[] \n",
    "for line in testuser: \n",
    "    line=line.replace('\\n','').split('\\t')\n",
    "  \n",
    "    clickids=[newsindex[x] for x in line[3].split()][-50:]\n",
    "    dwelltime=[int(x) for x in line[4].split()][-50:]\n",
    "    posfeedback=[clickids[x] for x in range(len(dwelltime)) if dwelltime[x] >time_T ]   \n",
    "    negfeedback=[clickids[x] for x in range(len(dwelltime)) if dwelltime[x] <=time_T ][:20]    \n",
    "    pdoc=[newsindex[x] for x in line[6].split()][:50]\n",
    "    ndoc=[newsindex[x] for x in line[7].split()][:300]\n",
    "    index=[]\n",
    "    index.append(len(test_sample)) \n",
    "    \n",
    "    for pos in pdoc:\n",
    "\n",
    "        test_sample.append(pos)\n",
    "        test_label.append(1)\n",
    "        test_user_pos.append(posfeedback+[0]*(50-len(posfeedback)))\n",
    "        test_user_neg.append(negfeedback+[0]*(20-len(negfeedback)))\n",
    "    for neg in ndoc:\n",
    "\n",
    "        test_sample.append(neg)\n",
    "        test_label.append(0)\n",
    "        test_user_pos.append(posfeedback+[0]*(50-len(posfeedback)))\n",
    "        test_user_neg.append(negfeedback+[0]*(20-len(negfeedback)))\n",
    "    index.append(len(test_sample))\n",
    "    test_index.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk-pFDiaS1Fo"
   },
   "outputs": [],
   "source": [
    "word_dict_raw={'PADDING':[0,999999]}\n",
    "\n",
    "for i in news:\n",
    "    for j in news[i][2]+news[i][3]:\n",
    "        if j in word_dict_raw:\n",
    "            word_dict_raw[j][1]+=1\n",
    "        else:\n",
    "            word_dict_raw[j]=[len(word_dict_raw),1] \n",
    "\n",
    "\n",
    "word_dict={}\n",
    "for i in word_dict_raw:\n",
    "    if word_dict_raw[i][1]>=5:\n",
    "        word_dict[i]=[len(word_dict),word_dict_raw[i][1]]\n",
    "print(len(word_dict),len(word_dict_raw))\n",
    "\n",
    "embdict={}\n",
    "with open('glove.840B.300d.txt','rb')as f:\n",
    "    linenb=0\n",
    "    while True:\n",
    "        line=f.readline()\n",
    "        if len(line)==0:\n",
    "            break\n",
    "        line = line.split()\n",
    "        word=line[0].decode()\n",
    "        linenb+=1\n",
    "        if len(word) != 0:\n",
    "            vec=[float(x) for x in line[1:]]\n",
    "            if word in word_dict:\n",
    "                embdict[word]=vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from numpy.linalg import cholesky  \n",
    "\n",
    "emb_mat=[0]*len(word_dict)\n",
    "in_dict_emb=[]\n",
    "for i in embdict.keys():\n",
    "    emb_mat[word_dict[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    in_dict_emb.append(emb_mat[word_dict[i][0]])\n",
    "in_dict_emb=np.array(in_dict_emb,dtype='float32')\n",
    "\n",
    "mu=np.mean(in_dict_emb, axis=0)\n",
    "Sigma=np.cov(in_dict_emb.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(emb_mat)):\n",
    "    if type(emb_mat[i])==int:\n",
    "        emb_mat[i]=np.reshape(norm, 300)\n",
    "emb_mat[0]=np.zeros(300,dtype='float32')\n",
    "emb_mat=np.array(emb_mat,dtype='float32')\n",
    "print(emb_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk-pFDiaS1Fo"
   },
   "outputs": [],
   "source": [
    "news_title=[[0]*30]\n",
    "\n",
    "for i in news:\n",
    "    line=[]\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict:\n",
    "            line.append(word_dict[j][0])\n",
    "    line=line[:30]\n",
    "    news_title.append(line+[0]*(30-len(line)))\n",
    "\n",
    "news_body=[[0]*200]\n",
    "\n",
    "for i in news:\n",
    "    line=[]\n",
    "    for j in news[i][3]:\n",
    "        if j in word_dict:\n",
    "            line.append(word_dict[j][0])\n",
    "    line=line[:200]\n",
    "    news_body.append(line+[0]*(200-len(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk-pFDiaS1Fo"
   },
   "outputs": [],
   "source": [
    "news_title=np.array(news_title,dtype='int32') \n",
    "news_body=np.array(news_body,dtype='int32') \n",
    "\n",
    "train_sample=np.array(train_sample,dtype='int32')\n",
    "label=np.array(label,dtype='int32')\n",
    "user_pos=np.array(user_pos,dtype='int32')\n",
    "user_neg=np.array(user_neg,dtype='int32')\n",
    "\n",
    "test_sample=np.array(test_sample,dtype='int32')\n",
    "test_label=np.array(test_label,dtype='int32')\n",
    "test_user_pos=np.array(test_user_pos,dtype='int32')\n",
    "test_user_neg=np.array(test_user_neg,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk-pFDiaS1Fo"
   },
   "outputs": [],
   "source": [
    "def generate_batch_data_random(batch_size):\n",
    "    idx = np.arange(len(label))\n",
    "    np.random.shuffle(idx)\n",
    "    y=label\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            cand_news_title = news_title[train_sample[i]]\n",
    "            cand_news_title=[cand_news_title[:,k,:] for k in range(cand_news_title.shape[1])]\n",
    "\n",
    "            user_title=news_title[user_pos[i]]\n",
    "            user_title=[user_title[:,k,:] for k in range(user_title.shape[1])]\n",
    " \n",
    "            user_title_neg=news_title[user_neg[i]]\n",
    "            user_title_neg=[user_title_neg[:,k,:] for k in range(user_title_neg.shape[1])]\n",
    "      \n",
    "            cand_news_body = news_body[train_sample[i]]\n",
    "            cand_news_body=[cand_news_body[:,k,:] for k in range(cand_news_body.shape[1])]\n",
    "            \n",
    "            user_body=news_body[user_pos[i]]\n",
    "            user_body=[user_body[:,k,:] for k in range(user_body.shape[1])]\n",
    "            \n",
    "            user_body_neg=news_body[user_neg[i]]\n",
    "            user_body_neg=[user_body_neg[:,k,:] for k in range(user_body_neg.shape[1])]\n",
    "             \n",
    "            yield (cand_news_title+user_title+user_title_neg+cand_news_body+user_body+user_body_neg, [label[i]])\n",
    "\n",
    "def generate_batch_data_test(batch_size):\n",
    "    \n",
    "\n",
    "    idx = np.arange(len(test_label))\n",
    "    y=test_label\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            cand_news_title = news_title[test_sample[i]]\n",
    "            \n",
    "            user_title=news_title[test_user_pos[i]]\n",
    "            user_title=[user_title[:,k,:] for k in range(user_title.shape[1])]\n",
    "            \n",
    "            user_title_neg=news_title[test_user_neg[i]]\n",
    "            user_title_neg=[user_title_neg[:,k,:] for k in range(user_title_neg.shape[1])]\n",
    "            \n",
    "            cand_news_body = news_body[test_sample[i]]\n",
    "            \n",
    "            user_body=news_body[test_user_pos[i]]\n",
    "            user_body=[user_body[:,k,:] for k in range(user_body.shape[1])]\n",
    "            user_body_neg=news_body[test_user_neg[i]]\n",
    "            user_body_neg=[user_body_neg[:,k,:] for k in range(user_body_neg.shape[1])] \n",
    "            \n",
    "            yield ([cand_news_title]+user_title+user_title_neg+[cand_news_body]+user_body+user_body_neg, [test_label[i]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk-pFDiaS1Fo"
   },
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk-pFDiaS1Fo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers \n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))    \n",
    "        A = K.softmax(A)\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk-pFDiaS1Fo"
   },
   "outputs": [],
   "source": [
    "results=[]\n",
    "keras.backend.clear_session()\n",
    "\n",
    "MAX_TITLE_LENGTH=30\n",
    "MAX_HIS_LEN=50\n",
    "MAX_NEGHIS_LEN=20\n",
    "MAX_BODY_LENGTH=200\n",
    "\n",
    "title_input = Input(shape=(MAX_TITLE_LENGTH,), dtype='int32')\n",
    "body_input = Input(shape=(MAX_BODY_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(10, 300,trainable=True)\n",
    "pos_embedding_layer_title = Embedding(MAX_TITLE_LENGTH+1, 300, trainable=True)(Lambda(lambda x:K.zeros_like(x,dtype='int32')+K.arange(x.shape[1]))(title_input))\n",
    "pos_embedding_layer_body = Embedding(MAX_BODY_LENGTH+1, 300, trainable=True)(Lambda(lambda x:K.zeros_like(x,dtype='int32')+K.arange(x.shape[1]))(body_input))\n",
    "\n",
    "embedded_sequences = embedding_layer(title_input)\n",
    "embedded_sequences = add([embedded_sequences,pos_embedding_layer_title])\n",
    "embedded_sequences = Dropout(0.2)(embedded_sequences)\n",
    "\n",
    "\n",
    "word_rep = Attention(8,32)([embedded_sequences]*3)\n",
    "word_rep=Dropout(0.2)(word_rep)\n",
    "\n",
    "attention = Dense(200,activation='tanh')(word_rep)\n",
    "attention = Flatten()(Dense(1)(attention))\n",
    "attention_weight = Activation('softmax')(attention)\n",
    "rep=Dot((1, 1))([word_rep, attention_weight])\n",
    "\n",
    "\n",
    "embedded_sequences2 = embedding_layer(body_input)\n",
    "embedded_sequences2 = add([embedded_sequences2,pos_embedding_layer_body])\n",
    "embedded_sequences2 = Dropout(0.2)(embedded_sequences2)\n",
    "\n",
    "word_rep2 = Attention(8,32)([embedded_sequences2]*3)\n",
    "word_rep2=Dropout(0.2)(word_rep2)\n",
    "\n",
    "attention2 = Dense(200,activation='tanh')(word_rep2)\n",
    "attention2 = Flatten()(Dense(1)(attention2))\n",
    "attention_weight2 = Activation('softmax')(attention2)\n",
    "rep2=Dot((1, 1))([word_rep2, attention_weight2])\n",
    "\n",
    "\n",
    "tb_attention = Dot((1, 2))([rep, word_rep2]) \n",
    "tb_attention_weight = Activation('softmax')(tb_attention)\n",
    "tb_rep=Dot((1, 1))([word_rep2, tb_attention_weight])\n",
    "\n",
    "bt_attention = Dot((1, 2))([rep2, word_rep]) \n",
    "bt_attention_weight = Activation('softmax')(bt_attention)\n",
    "bt_rep=Dot((1, 1))([word_rep, bt_attention_weight])\n",
    "\n",
    "news_rep=add([rep,rep2,tb_rep,bt_rep])\n",
    "\n",
    "news_encoder = Model([title_input,body_input], news_rep)\n",
    "\n",
    "\n",
    "his_input = [Input((MAX_TITLE_LENGTH,), dtype='int32') for _ in range(MAX_HIS_LEN)]\n",
    "his_input_body = [Input((MAX_BODY_LENGTH,), dtype='int32') for _ in range(MAX_HIS_LEN)]\n",
    "his_news_rep = [news_encoder([his_input[_],his_input_body[_]]) for _ in range(MAX_HIS_LEN)]\n",
    "his_news_rep = concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(doc) for doc in his_news_rep],axis=1)\n",
    "\n",
    "\n",
    "attention_user = Dense(200,activation='tanh')(his_news_rep)\n",
    "attention_user = Flatten()(Dense(1)(attention_user))\n",
    "attention_user_weight = Activation('softmax')(attention_user)\n",
    "user_rep=keras.layers.Dot((1, 1))([his_news_rep, attention_user_weight])\n",
    "\n",
    "his_input_neg = [Input((MAX_TITLE_LENGTH,), dtype='int32') for _ in range(MAX_NEGHIS_LEN)]\n",
    "his_input_neg_body = [Input((MAX_BODY_LENGTH,), dtype='int32') for _ in range(MAX_NEGHIS_LEN)]\n",
    "his_news_neg_rep =[news_encoder([his_input_neg[_],his_input_neg_body[_]]) for _ in range(MAX_NEGHIS_LEN)]\n",
    "his_news_neg_rep = concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(doc) for doc in his_news_neg_rep],axis=1)\n",
    "\n",
    "attention_user_neg = Dense(200,activation='tanh')(his_news_neg_rep)\n",
    "attention_user_neg = Flatten()(Dense(1)(attention_user_neg))\n",
    "attention_user_neg_weight = Activation('softmax')(attention_user_neg)\n",
    "user_rep_neg=keras.layers.Dot((1, 1))([his_news_neg_rep, attention_user_neg_weight])\n",
    "\n",
    "\n",
    "candidates =[Input((MAX_TITLE_LENGTH,), dtype='int32') for _ in range(1+npratio)]\n",
    "candidates_body =[Input((MAX_BODY_LENGTH,), dtype='int32') for _ in range(1+npratio)]\n",
    "candidate_vecs = [news_encoder([candidates[_],candidates_body[_]]) for _ in range(1+npratio)]\n",
    "candidate_vecs = concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(doc) for doc in candidate_vecs],axis=1)\n",
    "\n",
    "\n",
    "logits = keras.layers.dot([user_rep, candidate_vecs], axes=-1)\n",
    "logits_neg = keras.layers.dot([user_rep_neg, candidate_vecs], axes=-1)\n",
    "\n",
    "pnweight=Dense(1)\n",
    "logitspn=TimeDistributed(pnweight)(concatenate([Lambda(lambda x :K.expand_dims(x,axis=2))(logits),Lambda(lambda x :K.expand_dims(x,axis=2))(logits_neg)]))\n",
    "logits_final=Activation('softmax')(Flatten()(logitspn))\n",
    "\n",
    "model = Model(candidates+his_input+his_input_neg+candidates_body+his_input_body+his_input_neg_body, [logits_final])\n",
    "model.compile(loss=['categorical_crossentropy'], optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "candidate_one = keras.Input((MAX_TITLE_LENGTH,))\n",
    "candidate_one_body = keras.Input((MAX_BODY_LENGTH,))\n",
    "candidate_one_vec = news_encoder([candidate_one,candidate_one_body])\n",
    "        \n",
    "score = keras.layers.Activation(keras.activations.sigmoid)(\n",
    "           pnweight(concatenate([dot([user_rep, candidate_one_vec], axes=-1),dot([user_rep_neg, candidate_one_vec], axes=-1)])))\n",
    "model_test = keras.Model([candidate_one]+his_input+his_input_neg+[candidate_one_body]+his_input_body+his_input_neg_body, score)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for ep in range(2):\n",
    "    traingen=generate_batch_data_random(28)\n",
    "    model.fit_generator(traingen, epochs=1,steps_per_epoch=len(label)//28)\n",
    "    valgen=generate_batch_data_test(30)\n",
    "\n",
    "    auc=[]\n",
    "    mrr=[]\n",
    "    ndcg5=[]\n",
    "    ndcg10=[]\n",
    "    pred = model_test.predict_generator(valgen, steps=len(test_sample)//30,verbose=1)\n",
    "    for m in test_index:\n",
    "        if np.sum(test_label[m[0]:m[1]])!=0 and m[1]<len(cr):\n",
    "    \n",
    "            auc.append(roc_auc_score(test_label[m[0]:m[1]],pred[m[0]:m[1],0]))\n",
    "            mrr.append(mrr_score(test_label[m[0]:m[1]],pred[m[0]:m[1],0]))\n",
    "            ndcg5.append(ndcg_score(test_label[m[0]:m[1]],pred[m[0]:m[1],0],k=5))\n",
    "            ndcg10.append(ndcg_score(test_label[m[0]:m[1]],pred[m[0]:m[1],0],k=10))\n",
    "    results.append([np.mean(auc),np.mean(mrr),np.mean(ndcg5),np.mean(ndcg10)])\n",
    "    print(np.mean(auc),np.mean(mrr),np.mean(ndcg5),np.mean(ndcg10))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
